{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Hw1 report 0616215 \n",
    "## Github: https://github.com/Darklanx/deep_visual_recog_hw1\n",
    "I orginally use **resnet50** as the backbone of this assignment, however it is found that using a more complex network such as resnet101 can produce a better result.\n",
    "\n",
    "Later, a network that add an **attention mechanism** to the orginal resnet called **resnest** is used for more improvement: https://github.com/zhanghang1989/ResNeSt, in the end, I achieve my best score on Kaggle by using pretrained resnest200 and addition 21 epochs of transer learning.\n",
    "\n",
    "code reference: https://www.kaggle.com/deepbear/pytorch-car-classifier-90-accuracy\n",
    "\n",
    "Inorder to run resnest successfully, please run `pip3 install resnest --pre`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=0\n"
   ]
  },
  {
   "source": [
    "Fixing the random seed for reproducibility:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from Libs.Dataset import Dataset\n",
    "from Libs.Model import Net\n",
    "from Libs.train import train_model, eval_model\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import os \n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "source": [
    "Setting the batch size, note that if multiple GPUs are used, the batch_size should be divisible by the number of GPUs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30"
   ]
  },
  {
   "source": [
    "Setting the path to the data folder and the label file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_training = \"./data/training_data/training_data\"\n",
    "dir_testing = \"./data//testing_data/testing_data\"\n",
    "csv_file = \"data/training_labels.csv\""
   ]
  },
  {
   "source": [
    "Loading the csv file to obtain labels for the training set:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "label_ids = {}\n",
    "for label in df[\"label\"]:\n",
    "    if label not in label_ids:\n",
    "        label_ids[label] = len(label_ids)\n",
    "id_labels =  {v: k for k, v in label_ids.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preprocess\n",
    "For data preprocess, the training dataset is transformed with the following operation:\n",
    "\n",
    "1. resize to 400*400.\n",
    "\n",
    "2. random horizontal flip with p=0.5. (data augmentation)\n",
    "\n",
    "3. random rotation of +- 15 degree. (data augmentation)\n",
    "\n",
    "4. scale the pixel value from 0~255 to 0~1\n",
    "\n",
    "5. normalized with respect to each channel with mean and std both being 0.5.\n",
    "\n",
    "After applying the transformation, I split 10% of the training dataset out from the training dataset to the testing dataset, this testing dataset is used to adjust the learning rate during the training with pytorch's learning rate scheduler **ReduceLROnPlateau**, which will be further discussed in the training section.\n",
    "\n",
    "## Note\n",
    "Applying random horizontal flip is a method of data augmentation, and is crucial to the training of the model, when training the model with resnet50 as the backbone, applying random horizontal flip can boost the performance significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_trans = transforms.Compose([transforms.Resize((400, 400)),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.RandomRotation(15),\n",
    "                                 transforms.ToTensor(), # range [0, 255] -> [0.0,1.0]\n",
    "                                 transforms.Normalize((0.5), (0.5))])\n",
    "train_dataset = Dataset(dir_training, csv_file, label_ids, transform=train_trans)\n",
    "train_dataset, test_dataset = train_dataset.train_test_split(0.9)\n",
    "print(\"train dataset size: \", train_dataset.data.shape[0])\n",
    "print(\"test dataset size: \", test_dataset.data.shape[0])\n",
    "print(test_dataset.data[0])\n",
    "\n",
    "train_loader =  torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True,drop_last=False, num_workers=4)\n",
    "                               \n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "### Hyperparameters:\n",
    "1. Optimizer selection, and parameters of the optimizer (lr, momenetum etc..)\n",
    "\n",
    "2. Learning rate scheduler, parameters of the scheduler\n",
    "\n",
    "3. Number of training epoch\n",
    "\n",
    "4. Backbone selection: resnet50, resnet101, resnet200, resnest50, resnest101, resnest200.\n",
    "\n",
    "### Training details\n",
    "The model is trained with transfer learning by replacing the last fully connected layer from the pretrained **resnest200** to fit the task. \n",
    "#### Optimizer\n",
    "SGD with lr=0.01, momentum=0.9 is applied as the optimizer\n",
    "#### Learning Rate Scheduler\n",
    " Learning rate is scheduled by **ReduceLROnPlateau** scheduler, this scheduler takes a value as input after every epoch, and adjust the learning rate accordingly.\n",
    "\n",
    "In my training, I feed the accuracy of the model on the testing dataset as the input to the scheduler, and the learning rate will be multiply by 0.5 if the accuracy did not improve more than 0.8.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "I removed the epoch-by-ecoch outputs of the training (which contains the loss and testing error for every epoch) for simplicity, however TA can easiliy reproduce the result simply by running the code above. \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_EPOCH_LOAD = 0\n",
    "MODEL_DIR = \"./model/\"\n",
    "end_epoch = 21\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net(use_att=False)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    print(\"torch.cuda.device_count(): \", torch.cuda.device_count())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# lrscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "lrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5,patience=1, threshold = 0.8, threshold_mode='abs', min_lr=1e-4)\n",
    "\n",
    "\n",
    "if TRAIN_EPOCH_LOAD <= 0:\n",
    "    start_epoch = 0\n",
    "else:\n",
    "    start_epoch = TRAIN_EPOCH_LOAD\n",
    "    checkpoint = torch.load('{}.pth'.format(os.path.join(MODEL_DIR, str(TRAIN_EPOCH_LOAD))))\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if torch.is_tensor(v):\n",
    "                state[k] = v.to(device)\n",
    "\n",
    "    lrscheduler.step(checkpoint[\"test_acc\"])\n",
    "    # lrscheduler = checkpoint[\"scheduler\"]\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "model, training_losses, training_accs, test_accs = train_model(model, train_loader, test_loader, criterion, optimizer, lrscheduler, start_epoch, end_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "To produce the result for the submission, every input image is resized and normalized just as the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict = {}\n",
    "load_epoch = 15\n",
    "print(\"Testing...\")\n",
    "model.load_state_dict(torch.load('{}.pth'.format(os.path.join(\"./model/\", str(load_epoch))))[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"Test accuracy of epoch {}: {}\".format(load_epoch, eval_model(model, test_loader)))\n",
    "\n",
    "eval_trans = transforms.Compose([transforms.Resize((400, 400)),\n",
    "                                 transforms.ToTensor(), # range [0, 255] -> [0.0,1.0]\n",
    "                                 transforms.Normalize((0.5), (0.5))])\n",
    "eval_dataset =  Dataset(dir_testing, csv_file, label_ids=None, transform=eval_trans, eval=True)\n",
    "eval_loader =  torch.utils.data.DataLoader(eval_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=4) \n",
    "\n",
    "with torch.no_grad():\n",
    "    for b, inputs in enumerate(eval_loader):\n",
    "        imgs, img_names = inputs\n",
    "        imgs = imgs.to(device)\n",
    "        output = model(imgs)\n",
    "        p = torch.argmax(output, 1)\n",
    "        for i, img_name in enumerate(img_names):\n",
    "            predict[img_name] = id_labels[p[i].item()]\n",
    "\n",
    "with open('submission.csv', 'w',newline='') as csvfile:\n",
    "    fieldnames=[\"id\", \"label\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for key, value in predict.items():\n",
    "        writer.writerow({fieldnames[0]: key, fieldnames[1]: value})\n",
    "print(\"Done\")\n",
    "    "
   ]
  },
  {
   "source": [
    "# Discussion\n",
    "Several foundings was found during my experiments:\n",
    "\n",
    "1. The more complex the model is, the better result it produces. (No overfitting observed)\n",
    "\n",
    "2. Applying split-attention using resnet as backbone (resnest), I have observed improvement of 1~2%, however in the original paper (ResNeStï¼šSplit-Attention Networks) an improvement of 3~4% of improvement is observed, this may be affected by the setting of optimizer and lr scheduler.\n",
    "\n",
    "3. Adjusting learning rate with respect to testing accuracy/error has better performance than adjusting it by a fixed amount every epoch.\n",
    "\n",
    "4. Reducing the learning rate to a small value near the end of training can help stabilze the model and thus leading to a slight improvement. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}